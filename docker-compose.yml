version: "3.9"

services:
  # ── Python FastAPI backend ──────────────────────────────────────
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      AI_PROVIDER:              ${AI_PROVIDER:-openai}
      OPENAI_API_KEY:           ${OPENAI_API_KEY}
      OPENAI_MODEL:             ${OPENAI_MODEL:-gpt-4o}
      OPENAI_EMBEDDING_MODEL:   ${OPENAI_EMBEDDING_MODEL:-text-embedding-3-small}
      ANTHROPIC_API_KEY:        ${ANTHROPIC_API_KEY}
      ANTHROPIC_MODEL:          ${ANTHROPIC_MODEL:-claude-3-5-sonnet-20241022}
      OLLAMA_BASE_URL:          ${OLLAMA_BASE_URL:-http://ollama:11434}
      OLLAMA_MODEL:             ${OLLAMA_MODEL:-llama3.1}
      GITHUB_TOKEN:             ${GITHUB_TOKEN}
      CHROMA_PERSIST_DIR:       /app/chroma_db
    volumes:
      - chroma_data:/app/chroma_db
    restart: unless-stopped

  # ── Next.js frontend ────────────────────────────────────────────
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      BACKEND_URL: http://backend:8000
    depends_on:
      - backend
    restart: unless-stopped

  # ── Ollama (optional — only if AI_PROVIDER=ollama) ──────────────
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    profiles:
      - ollama                 # only starts with: docker compose --profile ollama up

volumes:
  chroma_data:
  ollama_data:
